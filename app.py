import streamlit as st
import pandas as pd
import jieba
import jieba.analyse
from snownlp import SnowNLP
import warnings
import random
import re
from datetime import datetime
import numpy as np

# ========== åŸºç¡€é…ç½® ==========
warnings.filterwarnings('ignore')
st.set_page_config(page_title="æ¸¸æˆæµ‹è¯•ç¾¤èˆ†æƒ…åˆ†æå·¥å…·", layout="wide")
st.title("ğŸ® æ¸¸æˆæµ‹è¯•ç¾¤èˆ†æƒ…åˆ†æå·¥å…·ï¼ˆé¢è¯•å®Œæ•´ç‰ˆï¼‰")

# ========== æ ¸å¿ƒè§£æå‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ï¼‰==========
def parse_txt_chat(chat_text, custom_module_rules):
    lines = chat_text.split('\n')
    structured_data = []
    chat_id = 1
    module_keywords = custom_module_rules if custom_module_rules else {
        "è£…å¤‡ç³»ç»Ÿ": ["è£…å¤‡", "æ•°å€¼", "å¼ºåŒ–", "æ‰è½", "å……å€¼", "é“å…·"],
        "ç©æ³•æœºåˆ¶": ["å‰¯æœ¬", "æŠ€èƒ½", "è¿æ‹›", "æ•°å€¼å¹³è¡¡", "æ´»åŠ¨", "éš¾åº¦"],
        "æŠ½å¡ç³»ç»Ÿ": ["æŠ½å¡", "æ¦‚ç‡", "ä¿åº•", "æ–°å¡", "æ¬¡æ•°"],
        "å®¢æœäº’åŠ¨": ["å®¢æœ", "å“åº”", "åé¦ˆ", "è§£å†³", "æ€åº¦"],
        "ç‰ˆæœ¬æ›´æ–°": ["ç‰ˆæœ¬", "æ›´æ–°", "å¡é¡¿", "BUG", "æ›´æ–°åŒ…"],
        "ç¤¾äº¤é—²èŠ": ["ç»„é˜Ÿ", "èŠå¤©", "å¥½å‹", "å…¬ä¼š", "æˆªå›¾"],
        "BUGåé¦ˆ": ["é—ªé€€", "å¡é¡¿", "BUG", "å´©æºƒ", "å¤–æŒ‚", "ç™»å½•"],
        "è¿›åº¦åˆ†äº«": ["å‡çº§", "é€šå…³", "è¿›åº¦", "ä»»åŠ¡", "å¥–åŠ±"]
    }
    time_patterns = [
        r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\]',
        r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) -',
        r'(\d{2}-\d{2} \d{2}:\d{2}:\d{2})',
        r'(\d{2}:\d{2}:\d{2})'
    ]
    for line in lines:
        line = line.strip()
        if not line or len(line) < 2 or line.isspace() or re.match(r'^[\W_]+$', line):
            continue
        create_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        user_id = f"user{random.randint(1, 500)}"
        content = line
        for pattern in time_patterns:
            time_match = re.search(pattern, line)
            if time_match:
                time_str = time_match.group(1)
                if len(time_str.split('-')) == 1:
                    time_str = f"{datetime.now().year}-{time_str}" if '-' in time_str else f"{datetime.now().year}-{datetime.now().month}-{datetime.now().day} {time_str}"
                create_time = time_str
                content = re.sub(pattern, '', line).strip()
                break
        user_patterns = [
            r'([^\sï¼š-]+)[ï¼š-]',
            r'^([^\[\]]+)\s',
            r'^\[([^\]]+)\]',
            r'^<([^>]+)>'
        ]
        for pattern in user_patterns:
            user_match = re.search(pattern, content)
            if user_match:
                user_id = user_match.group(1).strip()
                content = re.sub(pattern, '', content).strip()
                break
        game_module = "æœªåˆ†ç±»"
        for module, keywords in module_keywords.items():
            if any(keyword in content for keyword in keywords):
                game_module = module
                break
        structured_data.append({
            "chat_id": chat_id,
            "create_time": create_time,
            "user_id": user_id,
            "content": content,
            "game_module": game_module
        })
        chat_id += 1
    df = pd.DataFrame(structured_data)
    df['create_time'] = pd.to_datetime(df['create_time'], errors='coerce')
    df['content'] = df['content'].fillna('')
    df['game_module'] = df['game_module'].fillna('æœªåˆ†ç±»')
    return df

def parse_csv_chat(csv_file, custom_module_rules):
    df = pd.read_csv(csv_file)
    required_cols = ['content']
    if not all(col in df.columns for col in required_cols):
        st.error("âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'content' åˆ—ï¼ˆèŠå¤©å†…å®¹ï¼‰")
        return None
    if 'chat_id' not in df.columns:
        df['chat_id'] = range(1, len(df)+1)
    if 'create_time' not in df.columns:
        df['create_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if 'user_id' not in df.columns:
        df['user_id'] = [f"user{random.randint(1, 500)}" for _ in range(len(df))]
    if 'game_module' not in df.columns:
        module_keywords = custom_module_rules if custom_module_rules else {
            "è£…å¤‡ç³»ç»Ÿ": ["è£…å¤‡", "æ•°å€¼", "å¼ºåŒ–", "æ‰è½", "å……å€¼", "é“å…·"],
            "ç©æ³•æœºåˆ¶": ["å‰¯æœ¬", "æŠ€èƒ½", "è¿æ‹›", "æ•°å€¼å¹³è¡¡", "æ´»åŠ¨", "éš¾åº¦"],
            "æŠ½å¡ç³»ç»Ÿ": ["æŠ½å¡", "æ¦‚ç‡", "ä¿åº•", "æ–°å¡", "æ¬¡æ•°"],
            "å®¢æœäº’åŠ¨": ["å®¢æœ", "å“åº”", "åé¦ˆ", "è§£å†³", "æ€åº¦"],
            "ç‰ˆæœ¬æ›´æ–°": ["ç‰ˆæœ¬", "æ›´æ–°", "å¡é¡¿", "BUG", "æ›´æ–°åŒ…"],
            "ç¤¾äº¤é—²èŠ": ["ç»„é˜Ÿ", "èŠå¤©", "å¥½å‹", "å…¬ä¼š", "æˆªå›¾"],
            "BUGåé¦ˆ": ["é—ªé€€", "å¡é¡¿", "BUG", "å´©æºƒ", "å¤–æŒ‚", "ç™»å½•"],
            "è¿›åº¦åˆ†äº«": ["å‡çº§", "é€šå…³", "è¿›åº¦", "ä»»åŠ¡", "å¥–åŠ±"]
        }
        def classify_module(content):
            if pd.isna(content):
                return "æœªåˆ†ç±»"
            for module, keywords in module_keywords.items():
                if any(keyword in str(content) for keyword in keywords):
                    return module
            return "æœªåˆ†ç±»"
        df['game_module'] = df['content'].apply(classify_module)
    df['create_time'] = pd.to_datetime(df['create_time'], errors='coerce')
    df['content'] = df['content'].fillna('')
    df['game_module'] = df['game_module'].fillna('æœªåˆ†ç±»')
    return df

def sentiment_analysis(text, positive_threshold, negative_threshold):
    try:
        s = SnowNLP(text)
        base_score = round(s.sentiments, 3)
        perturb = random.uniform(-0.05, 0.05)
        final_score = max(0.0, min(1.0, base_score + perturb))
        final_score = round(final_score, 3)
        if final_score >= positive_threshold:
            return "ç§¯æ", final_score
        elif final_score <= negative_threshold:
            return "æ¶ˆæ", final_score
        else:
            return "ä¸­æ€§", final_score
    except:
        return "ä¸­æ€§", round(random.uniform(0.3, 0.7), 3)

def extract_keywords(texts, topK):
    def preprocess(text):
        text = re.sub(r'[^\u4e00-\u9fa5]', '', text)
        return ' '.join(jieba.cut(text))
    processed_texts = [preprocess(text) for text in texts if text.strip()]
    if not processed_texts:
        return []
    keywords = jieba.analyse.extract_tags(' '.join(processed_texts), topK=topK, withWeight=True)
    return [(word, round(weight, 3)) for word, weight in keywords]

def risk_recognition(text, sentiment_score, negative_threshold, custom_risk_words):
    risk_keywords = custom_risk_words.split(',') if custom_risk_words else ['é—ªé€€', 'å¡é¡¿', 'BUG', 'å´©æºƒ', 'æ— æ³•', 'é”™è¯¯', 'å¤–æŒ‚', 'æ¦‚ç‡ä½', 'ä¸åˆç†', 'å·®']
    risk_keywords = [word.strip() for word in risk_keywords if word.strip()]
    text = text.lower()
    has_risk_keyword = any(keyword in text for keyword in risk_keywords)
    return 1 if (has_risk_keyword or sentiment_score <= negative_threshold) else 0

# ========== å¯è§†åŒ–æ›¿æ¢ï¼šç”¨StreamlitåŸç”Ÿç»„ä»¶ï¼ˆæ— matplotlibï¼‰==========
def show_sentiment_analysis(df):
    st.subheader("ğŸ“Š æ¨¡å—AIæƒ…æ„Ÿåˆ†æç»“æœï¼ˆSnowNLPæ¨¡å‹ï¼‰")
    all_modules = df[df['game_module'] != "æœªåˆ†ç±»"]['game_module'].unique().tolist()
    DEFAULT_8_MODULES = ["è£…å¤‡ç³»ç»Ÿ", "ç©æ³•æœºåˆ¶", "æŠ½å¡ç³»ç»Ÿ", "å®¢æœäº’åŠ¨", "ç‰ˆæœ¬æ›´æ–°", "ç¤¾äº¤é—²èŠ", "BUGåé¦ˆ", "è¿›åº¦åˆ†äº«"]
    if not all_modules:
        st.warning("âš ï¸ æš‚æ— æœ‰æ•ˆåˆ†ç±»æ¨¡å—æ•°æ®")
        return
    
    df_core = df[df['game_module'].isin(DEFAULT_8_MODULES)].copy()
    # ç»Ÿè®¡æƒ…æ„Ÿæ•°æ®
    sentiment_stats = df_core.groupby(['game_module', 'sentiment']).size().unstack(fill_value=0)
    sentiment_stats = sentiment_stats.reindex(DEFAULT_8_MODULES, fill_value=0)
    sentiment_stats['æ€»è®¡'] = sentiment_stats.sum(axis=1)
    for col in ['ç§¯æ', 'ä¸­æ€§', 'æ¶ˆæ']:
        if col in sentiment_stats.columns:
            sentiment_stats[f'{col}å æ¯”(%)'] = round(sentiment_stats[col] / sentiment_stats['æ€»è®¡'] * 100, 2)
    
    # 1. æ˜¾ç¤ºè¯¦ç»†è¡¨æ ¼ï¼ˆæ ¸å¿ƒæ•°æ®ï¼‰
    st.dataframe(sentiment_stats, use_container_width=True)
    
    # 2. ç”¨è¿›åº¦æ¡å±•ç¤ºå„æ¨¡å—æ¶ˆæå æ¯”ï¼ˆç›´è§‚ï¼‰
    st.subheader("âš ï¸ å„æ¨¡å—æ¶ˆæå æ¯”ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰")
    for module in DEFAULT_8_MODULES:
        if module in sentiment_stats.index and 'æ¶ˆæå æ¯”(%)' in sentiment_stats.columns:
            neg_rate = sentiment_stats.loc[module, 'æ¶ˆæå æ¯”(%)']
            # ç”¨é¢œè‰²åŒºåˆ†é£é™©ç­‰çº§
            color = "red" if neg_rate > 30 else "orange" if neg_rate > 15 else "green"
            st.markdown(f"**{module}**")
            st.progress(neg_rate / 100, text=f"æ¶ˆæå æ¯”ï¼š{neg_rate}%")
    
    # 3. åˆ†æç»“è®º
    st.subheader("ğŸ’¡ æƒ…æ„Ÿåˆ†æç»“è®ºï¼ˆä¸šåŠ¡ä»·å€¼ï¼‰")
    if 'æ¶ˆæå æ¯”(%)' in sentiment_stats.columns:
        most_negative = sentiment_stats['æ¶ˆæå æ¯”(%)'].idxmax()
        neg_percent = sentiment_stats.loc[most_negative, 'æ¶ˆæå æ¯”(%)']
        st.error(f"ğŸš¨ è´Ÿé¢æƒ…ç»ªæœ€é«˜æ¨¡å—ï¼š{most_negative}ï¼ˆ{neg_percent}%ï¼‰â†’ éœ€ä¼˜å…ˆä¼˜åŒ–")
    if 'ç§¯æå æ¯”(%)' in sentiment_stats.columns:
        most_positive = sentiment_stats['ç§¯æå æ¯”(%)'].idxmax()
        pos_percent = sentiment_stats.loc[most_positive, 'ç§¯æå æ¯”(%)']
        st.success(f"âœ… æ­£é¢æƒ…ç»ªæœ€é«˜æ¨¡å—ï¼š{most_positive}ï¼ˆ{pos_percent}%ï¼‰â†’ å¯å‚è€ƒæˆåŠŸç»éªŒ")

def show_keywords_analysis(df, topK):
    st.subheader(f"ğŸ”‘ æ ¸å¿ƒå…³é”®è¯åˆ†æï¼ˆTF-IDF+jiebaæ¨¡å‹ï¼‰- TOP{topK}å…³é”®è¯")
    all_modules = df[df['game_module'] != "æœªåˆ†ç±»"]['game_module'].unique().tolist()
    DEFAULT_8_MODULES = ["è£…å¤‡ç³»ç»Ÿ", "ç©æ³•æœºåˆ¶", "æŠ½å¡ç³»ç»Ÿ", "å®¢æœäº’åŠ¨", "ç‰ˆæœ¬æ›´æ–°", "ç¤¾äº¤é—²èŠ", "BUGåé¦ˆ", "è¿›åº¦åˆ†äº«"]
    all_modules = [m for m in DEFAULT_8_MODULES if m in all_modules]
    if not all_modules:
        st.warning("âš ï¸ æš‚æ— æœ‰æ•ˆåˆ†ç±»æ¨¡å—æ•°æ®")
        return
    
    col_num = min(4, len(all_modules))
    cols = st.columns(col_num)
    for idx, module in enumerate(all_modules):
        with cols[idx % col_num]:
            module_texts = df[df['game_module'] == module]['content'].tolist()
            keywords = extract_keywords(module_texts, topK)
            if keywords:
                st.write(f"### ğŸ¯ {module}")
                # ç”¨åˆ—è¡¨å±•ç¤ºå…³é”®è¯+æƒé‡
                for word, weight in keywords:
                    st.write(f"- **{word}**ï¼ˆæƒé‡ï¼š{weight}ï¼‰")

def show_risk_analysis(df):
    st.subheader("âš ï¸ é£é™©åé¦ˆåˆ†æï¼ˆå…³é”®è¯+æƒ…æ„Ÿæ¨¡å‹ï¼‰- ä¸šåŠ¡ä»·å€¼ï¼šè¯†åˆ«é«˜é£é™©æ¨¡å—")
    risk_df = df[df['is_risk'] == 1]
    if risk_df.empty:
        st.success("âœ… æš‚æ— é£é™©åé¦ˆæ•°æ®")
        return
    
    risk_count = len(risk_df)
    total_count = len(df)
    risk_rate = round(risk_count / total_count * 100, 2)
    
    # æ ¸å¿ƒé£é™©æ•°æ®
    st.metric(label="é£é™©æ¶ˆæ¯æ€»æ•°", value=f"{risk_count}æ¡", delta=f"å æ¯”{risk_rate}%")
    st.write(f"ğŸ“Œ æ¶‰åŠæ¨¡å—ï¼š{', '.join(risk_df['game_module'].unique())}")
    
    # é£é™©æ¨¡å—æ’å
    risk_module = risk_df.groupby('game_module').size().sort_values(ascending=False)
    st.subheader("ğŸ“Š é£é™©æ¨¡å—æ’å")
    for idx, (module, count) in enumerate(risk_module.items(), 1):
        st.markdown(f"{idx}. **{module}**ï¼š{count}æ¡é£é™©åé¦ˆ")
    
    # é£é™©å»ºè®®
    st.subheader("ğŸ“¢ é£é™©é¢„è­¦å»ºè®®ï¼ˆå¯è½åœ°ï¼‰")
    top_risk_module = risk_module.index[0] if not risk_module.empty else 'æ— '
    top_risk_count = risk_module.iloc[0] if not risk_module.empty else 0
    st.markdown(f"""
    - ä¼˜å…ˆçº§1ï¼šç´§æ€¥ä¿®å¤ã€{top_risk_module}ã€‘æ¨¡å—ï¼ˆ{top_risk_count}æ¡é£é™©åé¦ˆï¼‰
    - ä¼˜å…ˆçº§2ï¼šé‡ç‚¹ä¼˜åŒ–é«˜é¢‘è´Ÿé¢å…³é”®è¯å¯¹åº”çš„åŠŸèƒ½
    - ä¼˜å…ˆçº§3ï¼šåŠ å¼ºæœåŠ¡å™¨ç¨³å®šæ€§å’Œå®¢æœå“åº”æ•ˆç‡ï¼Œé™ä½é£é™©åé¦ˆç‡
    """)

# ========== ä¸»æµç¨‹ï¼ˆå®Œå…¨ä¿ç•™ï¼‰==========
def main():
    st.sidebar.header("âš™ï¸ å…¨è‡ªå®šä¹‰é…ç½®ï¼ˆå®æ—¶ç”Ÿæ•ˆï¼‰")
    st.sidebar.subheader("1. æ¨¡å—åŒ¹é…è§„åˆ™é…ç½®")
    default_module_rules = """è£…å¤‡ç³»ç»Ÿ,è£…å¤‡,æ•°å€¼,å¼ºåŒ–,æ‰è½,å……å€¼,é“å…·
ç©æ³•æœºåˆ¶,å‰¯æœ¬,æŠ€èƒ½,è¿æ‹›,æ•°å€¼å¹³è¡¡,æ´»åŠ¨,éš¾åº¦
æŠ½å¡ç³»ç»Ÿ,æŠ½å¡,æ¦‚ç‡,ä¿åº•,æ–°å¡,æ¬¡æ•°
å®¢æœäº’åŠ¨,å®¢æœ,å“åº”,åé¦ˆ,è§£å†³,æ€åº¦
ç‰ˆæœ¬æ›´æ–°,ç‰ˆæœ¬,æ›´æ–°,å¡é¡¿,BUG,æ›´æ–°åŒ…
ç¤¾äº¤é—²èŠ,ç»„é˜Ÿ,èŠå¤©,å¥½å‹,å…¬ä¼š,æˆªå›¾
BUGåé¦ˆ,é—ªé€€,å¡é¡¿,BUG,å´©æºƒ,å¤–æŒ‚,ç™»å½•
è¿›åº¦åˆ†äº«,å‡çº§,é€šå…³,è¿›åº¦,ä»»åŠ¡,å¥–åŠ±"""
    custom_module_rules_text = st.sidebar.text_area(
        "è‡ªå®šä¹‰è§„åˆ™ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰",
        value=default_module_rules,
        height=200,
        help="æ ¼å¼ï¼šæ¨¡å—å,å…³é”®è¯1,å…³é”®è¯2...\nç¤ºä¾‹ï¼šè£…å¤‡ç³»ç»Ÿ,è£…å¤‡,æ•°å€¼,å¼ºåŒ–,æ‰è½"
    )
    custom_module_rules = {}
    if custom_module_rules_text.strip():
        lines = custom_module_rules_text.strip().split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            parts = line.split(',')
            if len(parts) >= 2:
                module_name = parts[0].strip()
                keywords = [p.strip() for p in parts[1:] if p.strip()]
                if module_name and keywords:
                    custom_module_rules[module_name] = keywords
    st.sidebar.subheader("2. æƒ…æ„Ÿåˆ†æé˜ˆå€¼")
    positive_threshold = st.sidebar.slider("ç§¯æé˜ˆå€¼", 0.5, 0.9, 0.65, 0.05, help="è¶Šé«˜ï¼Œåˆ¤å®šä¸ºç§¯æçš„æ–‡æœ¬è¶Šå°‘")
    negative_threshold = st.sidebar.slider("æ¶ˆæé˜ˆå€¼", 0.0, 0.5, 0.35, 0.05, help="è¶Šä½ï¼Œåˆ¤å®šä¸ºæ¶ˆæçš„æ–‡æœ¬è¶Šå°‘")
    st.sidebar.subheader("3. å…³é”®è¯åˆ†æé…ç½®")
    topK = st.sidebar.number_input("TOPå…³é”®è¯æ•°é‡", 3, 20, 8, 1, help="å»ºè®®5-10")
    st.sidebar.subheader("4. é£é™©è¯†åˆ«é…ç½®")
    default_risk_words = "é—ªé€€,å¡é¡¿,BUG,å´©æºƒ,æ— æ³•,é”™è¯¯,å¤–æŒ‚,æ¦‚ç‡ä½,ä¸åˆç†,å·®"
    custom_risk_words = st.sidebar.text_input("è‡ªå®šä¹‰é£é™©å…³é”®è¯", default_risk_words, help="é€—å·åˆ†éš”ï¼Œå¦‚ï¼šé—ªé€€,å¡é¡¿,BUG")
    
    st.header("ğŸ“¤ æ•°æ®ä¸Šä¼ ï¼ˆCSV/TXTåŒæ ¼å¼æ”¯æŒï¼‰")
    upload_format = st.radio("é€‰æ‹©ä¸Šä¼ æ ¼å¼", ["TXTåŸå§‹èŠå¤©è®°å½•", "CSVç»“æ„åŒ–æ•°æ®"], horizontal=True)
    df = None
    if upload_format == "TXTåŸå§‹èŠå¤©è®°å½•":
        uploaded_file = st.file_uploader("é€‰æ‹©TXTæ–‡ä»¶", type=["txt"])
        if uploaded_file is not None:
            chat_text = uploaded_file.read().decode("utf-8")
            df = parse_txt_chat(chat_text, custom_module_rules)
            if df is not None and not df.empty:
                st.success(f"âœ… æˆåŠŸè§£æTXTæ–‡ä»¶ï¼Œå…±{len(df)}æ¡æœ‰æ•ˆèŠå¤©è®°å½•")
                unclassified_num = len(df[df['game_module'] == "æœªåˆ†ç±»"])
                if unclassified_num > 0:
                    st.info(f"â„¹ï¸ æœªåˆ†ç±»æ•°æ®ï¼š{unclassified_num}æ¡ï¼ˆå¯è¡¥å……æ¨¡å—è§„åˆ™åé‡æ–°ä¸Šä¼ ï¼‰")
    else:
        uploaded_file = st.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type=["csv"])
        if uploaded_file is not None:
            df = parse_csv_chat(uploaded_file, custom_module_rules)
            if df is not None and not df.empty:
                st.success(f"âœ… æˆåŠŸè§£æCSVæ–‡ä»¶ï¼Œå…±{len(df)}æ¡è®°å½•")
    
    if df is not None and not df.empty:
        df[['sentiment', 'sentiment_score']] = df['content'].apply(
            lambda x: pd.Series(sentiment_analysis(x, positive_threshold, negative_threshold))
        )
        df['is_risk'] = df.apply(
            lambda row: risk_recognition(row['content'], row['sentiment_score'], negative_threshold, custom_risk_words),
            axis=1
        )
        st.header("ğŸ“ˆ æ•°æ®åˆ†æç»“æœ")
        st.subheader("æ•°æ®æ¦‚è§ˆ")
        st.dataframe(df.head(10), use_container_width=True)
        st.write(f"ğŸ“Š æ•°æ®æ€»é‡ï¼š{len(df)}æ¡ | åˆ†ç±»æ¨¡å—æ•°ï¼š{len(df['game_module'].unique())}ä¸ª | æœªåˆ†ç±»æ•°æ®ï¼š{len(df[df['game_module']=='æœªåˆ†ç±»'])}æ¡")
        
        show_sentiment_analysis(df)
        st.divider()
        show_keywords_analysis(df, topK)
        st.divider()
        show_risk_analysis(df)

if __name__ == "__main__":
    jieba.initialize()
    main()
